---
description: Amber's Roadmap For Becoming an NLP Expert in 4 Months with Suhas
cover: >-
  https://images.unsplash.com/photo-1528605248644-14dd04022da1?crop=entropy&cs=tinysrgb&fm=jpg&ixid=MnwxOTcwMjR8MHwxfHNlYXJjaHwxMHx8dGVhbSUyMG9mJTIwcGVvcGxlfGVufDB8fHx8MTY2MDMxNzQzNg&ixlib=rb-1.2.1&q=80
coverY: 0
---

# ðŸ‘©ðŸ’» NLP Roadmap

{% embed url="https://docs.google.com/document/d/1qLeVbPE_ICVSeNkytx9ly_y4r3BROhAvAYadj1WNjCc/edit" %}
Amber's Roadmap to Success
{% endembed %}

## Curriculum:&#x20;

<details>

<summary>Machine Learning Resources</summary>

* [https://deep-learning-drizzle.github.io/](https://deep-learning-drizzle.github.io/)&#x20;
* Neural Networks for Machine Learning
* Hugo Larochelle, UniversitÃ© de Sherbrooke
* Yann LeCun and Alfredo Canziani, NYU
* Pieter Abbeel, Sergey Karayev, UC Berkeley
* Machine Learning with Graphs (Jure Leskovec, Stanford)&#x20;
* Deep Learning and Reinforcement Learning Summer School (Lots of Legends, AMII, Edmonton, Canada)&#x20;

</details>

<details>

<summary>NLP Resources</summary>

* NLP ([http://www.phontron.com/class/nn4nlp2021/](http://www.phontron.com/class/nn4nlp2021/))&#x20;
* NLTK&#x20;
* SpaCy
* Gensim&#x20;
* FastText
*

</details>

*
*
*
* Machine Learning Resources:
*
*
  * [http://www.phontron.com/class/nn4nlp2021/schedule.html](http://www.phontron.com/class/nn4nlp2021/schedule.html)&#x20;
* NLP Tools and Resources:&#x20;
*
  * [https://www.nltk.org/](https://www.nltk.org/)&#x20;
  *
    * Not SOTA anymore but one of the oldest libraries; extremely good in a lot of things that are sometimes overlooked
    *
      * Sentence tokenization (punkt)&#x20;
      *
        * Unsupervised punkt training (using new domain)&#x20;
      * Finding concordances&#x20;
    * [https://www.nltk.org/book/](https://www.nltk.org/book/)&#x20;
  * [https://spacy.io/](https://spacy.io/)&#x20;
  *
    * Pros:&#x20;
    *
      * POS tagging + grammar (tense, voice, etc) â†’ doesnâ€™t have full coverage but is pretty good&#x20;
      * NER - where spacy is used the most&#x20;
      * Low level grammar stuff (morphology)&#x20;
    * [https://course.spacy.io/en/](https://course.spacy.io/en/)&#x20;
  * [https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)&#x20;
  *
    * Topic modeling&#x20;
    * Word2vec, Glove, etc
  * [https://fasttext.cc/](https://fasttext.cc/)&#x20;
  *
    * Embeddings&#x20;
  * [https://www.sbert.net/](https://www.sbert.net/)&#x20;
  *
    * Army knife of NLP&#x20;
    * Clustering!!&#x20;
    * Unsupervised training of SBERT
  * [https://pypi.org/project/textdistance/](https://pypi.org/project/textdistance/)&#x20;
  * [https://nlp.stanford.edu/software/openie.html](https://nlp.stanford.edu/software/openie.html)&#x20;
  *
    * Find the python version
    * Relationship analysis&#x20;
    * Information extraction
    * [https://github.com/philipperemy/stanford-openie-python](https://github.com/philipperemy/stanford-openie-python)&#x20;
  * [https://github.com/thunlp/OpenNRE](https://github.com/thunlp/OpenNRE)&#x20;
  *
    * Named relationship extraction&#x20;
  * [https://github.com/bhoov/exbert](https://github.com/bhoov/exbert)&#x20;
  * [https://github.com/MaartenGr/BERTopic](https://github.com/MaartenGr/BERTopic)&#x20;
  *
    * Topic modeling using bert&#x20;
  * [https://github.com/kpu/kenlm](https://github.com/kpu/kenlm)&#x20;
  *
    * Perplexity (element of surprise)&#x20;
    *
      * Low is good, high is bad
      * Surprised a lot = poorer model of the word&#x20;
  * [https://github.com/jamesturk/jellyfish](https://github.com/jamesturk/jellyfish)&#x20;
  *
    * Similar to text distance but faster&#x20;
  * [https://github.com/dsfsi/textaugment](https://github.com/dsfsi/textaugment)&#x20;
  *
    * Text augmentation&#x20;
* Self Learning Resources:&#x20;
* (1) [https://missing.csail.mit.edu/](https://missing.csail.mit.edu/)&#x20;
*
  * [https://missing.csail.mit.edu/2020/version-control/](https://missing.csail.mit.edu/2020/version-control/)&#x20;
  * [https://missing.csail.mit.edu/2020/data-wrangling/](https://missing.csail.mit.edu/2020/data-wrangling/)&#x20;
  * [https://missing.csail.mit.edu/2020/shell-tools/](https://missing.csail.mit.edu/2020/shell-tools/)&#x20;
  * [https://missing.csail.mit.edu/2020/debugging-profiling/](https://missing.csail.mit.edu/2020/debugging-profiling/)&#x20;
  * [https://missing.csail.mit.edu/2020/metaprogramming/](https://missing.csail.mit.edu/2020/metaprogramming/)&#x20;
  * [https://missing.csail.mit.edu/2020/course-shell/](https://missing.csail.mit.edu/2020/course-shell/) - only check the end :) &#x20;
* (2) CI/CD
*
  * [https://lab.github.com/](https://lab.github.com/)&#x20;
  * [https://lab.github.com/githubtraining/continuous-integration-with-circleci](https://lab.github.com/githubtraining/continuous-integration-with-circleci)&#x20;
  *
    * Circle ci = product&#x20;
    * Probably more popular but similar to travis&#x20;
  * [https://lab.github.com/githubtraining/continuous-integration-with-travis-ci](https://lab.github.com/githubtraining/continuous-integration-with-travis-ci)
  * [https://lab.github.com/githubtraining/devops-with-github-actions](https://lab.github.com/githubtraining/devops-with-github-actions)&#x20;
  *
    * How to tie things all together
    * Do this with one of the initial projects&#x20;
