# Lecture 2.4 - Why the learning works

{% embed url="https://www.youtube.com/watch?ab_channel=ColinReckons&index=9&list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9&v=d6nFTN081S8" %}

* How can we look at a proof of the perceptron learning procedure?&#x20;
* Can we use our geometric understanding of weights to prove that the perceptron will eventually find a weight vector that will find the right answers? (i.e. does that feasible vector always exist?)&#x20;
  * we can update the weight vector anytime we get a training case wrong&#x20;

<figure><img src="../../.gitbook/assets/Screen Shot 2023-06-05 at 2.38.45 PM.png" alt=""><figcaption></figcaption></figure>

<figure><img src="../../.gitbook/assets/Screen Shot 2023-06-05 at 2.40.00 PM.png" alt=""><figcaption></figcaption></figure>

<figure><img src="../../.gitbook/assets/Screen Shot 2023-06-05 at 2.42.48 PM.png" alt=""><figcaption><p>This depends on the assumption that there is a generously feasible weight vector</p></figcaption></figure>

